{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\n",
    "import re, os, json, codecs, gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_root_path='/home/aistudio/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17042\n",
       "1    17010\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/data21035/train.csv\")\n",
    "\n",
    "data = data.drop_duplicates(\n",
    "    subset=['text', 'piclist', 'userGender', 'userFollowCount', 'userFansCount', 'userWeiboCount', 'userLocation',\n",
    "            'userDescription']).reset_index()\n",
    "train_size = len(data)\n",
    "\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userFollowCount', '用户关注数']\n",
      "['userFansCount', '粉丝数']\n",
      "['userWeiboCount', '发布微博数']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京朝阳区@@文体娱乐@@图片个数0@@用户关注数1728@@粉丝数748@@发布微博数30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>云南楚雄@@社会生活@@图片个数1@@用户关注数423@@粉丝数112@@发布微博数792@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>没有地址@@社会生活@@图片个数1@@用户关注数0@@粉丝数0@@发布微博数0@@没有描述</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>北京东城区@@社会生活@@图片个数1@@用户关注数1668@@粉丝数7470000@@发布微...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>江苏盐城@@社会生活@@图片个数0@@用户关注数267@@粉丝数61@@发布微博数1098@...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               textb\n",
       "0  北京朝阳区@@文体娱乐@@图片个数0@@用户关注数1728@@粉丝数748@@发布微博数30...\n",
       "1  云南楚雄@@社会生活@@图片个数1@@用户关注数423@@粉丝数112@@发布微博数792@...\n",
       "2      没有地址@@社会生活@@图片个数1@@用户关注数0@@粉丝数0@@发布微博数0@@没有描述\n",
       "3  北京东城区@@社会生活@@图片个数1@@用户关注数1668@@粉丝数7470000@@发布微...\n",
       "4  江苏盐城@@社会生活@@图片个数0@@用户关注数267@@粉丝数61@@发布微博数1098@..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['textb'] = data['userLocation'].fillna('没有地址')\n",
    "data['userDescription'] = \"@@\" + data['userDescription'].fillna('没有描述')\n",
    "data['category'] = \"@@\" + data['category'].fillna('没有分类')\n",
    "\n",
    "data['piclist'] = data['piclist'].fillna('')\n",
    "data['pic_size'] = data['piclist'].apply(lambda x: str(len(x.split(\"\\t\"))) if x else '0')\n",
    "data['pic_size'] = \"@@图片个数\" + data['pic_size']\n",
    "data['textb'] = data['textb'] + data['category'] + data['pic_size']\n",
    "\n",
    "for col_name in [['userFollowCount', '用户关注数'], ['userFansCount', '粉丝数'], ['userWeiboCount', '发布微博数']]:\n",
    "    print(col_name)\n",
    "    col = col_name[0]\n",
    "    name = col_name[1]\n",
    "    data[col] = data[col].fillna(0).map(int)\n",
    "    name = \"@@\" + name\n",
    "    data[col] = name + data[col].map(str)\n",
    "    data['textb'] = data['textb'] + data[col]\n",
    "\n",
    "data['textb'] = data['textb'] + data['userDescription']\n",
    "\n",
    "data[['textb']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_root_path + 'data.txt', 'w') as f:\n",
    "    for row,i in data.iterrows():\n",
    "        f.write(i[\"text\"]+\"$@@@@@\" + str(int(i[\"label\"])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n",
      "数据列表生成完成！\n"
     ]
    }
   ],
   "source": [
    "def create_data_list(data_root_path):\n",
    "    with open(data_root_path + 'test_list.txt', 'w') as f:\n",
    "        pass\n",
    "    with open(data_root_path + 'train_list.txt', 'w') as f:\n",
    "        pass\n",
    "\n",
    "    with open(os.path.join(data_root_path, 'dict_txt.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "\n",
    "    with open(os.path.join(data_root_path, 'data.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        title = line.split(\"$@@@@@\")[0].replace('\\n', '')\n",
    "        l = line.split(\"$@@@@@\")[1]\n",
    "        labs = \"\"\n",
    "        if i % 10 == 0:\n",
    "            with open(os.path.join(data_root_path, 'test_list.txt'), 'a', encoding='utf-8') as f_test:\n",
    "                for s in title:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + l\n",
    "                f_test.write(labs)\n",
    "        else:\n",
    "            with open(os.path.join(data_root_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "                for s in title:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + l\n",
    "                f_train.write(labs)\n",
    "        i += 1\n",
    "    print(\"数据列表生成完成！\")\n",
    "\n",
    "\n",
    "# 把下载得数据生成一个字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    dict_set = set()\n",
    "    # 读取已经下载得数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        title = line.replace('\\n', '')\n",
    "        for s in title:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "    print(\"数据字典生成完成！\")\n",
    "\n",
    "\n",
    "# 获取字典的长度\n",
    "def get_dict_len(dict_path):\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        line = eval(f.readlines()[0])\n",
    "\n",
    "    return len(line.keys())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 把生产的数据列表都放在自己的总类别文件夹中\n",
    "    data_root_path = \"/home/aistudio/data/\"\n",
    "    data_path = os.path.join(data_root_path, 'data.txt')\n",
    "    dict_path = os.path.join(data_root_path, \"dict_txt.txt\")\n",
    "    # 创建数据字典\n",
    "    create_dict(data_path, dict_path)\n",
    "    # 创建数据列表\n",
    "    create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建数据读取器train_reader 和test_reader\n",
    "# 训练/测试数据的预处理\n",
    "def data_mapper(sample):\n",
    "    data, label = sample\n",
    "    data = [int(data) for data in data.split(',')]\n",
    "    return data, int(label)\n",
    "\n",
    "# 创建数据读取器train_reader\n",
    "def train_reader(train_list_path):\n",
    "    def reader():\n",
    "        with open(train_list_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # 打乱数据\n",
    "            np.random.shuffle(lines)\n",
    "            # 开始获取每张图像和标签\n",
    "            for line in lines:\n",
    "                data, label = line.split('\\t')\n",
    "                yield data, label\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)\n",
    "#  创建数据读取器test_reader\n",
    "def test_reader(test_list_path):\n",
    "\n",
    "    def reader():\n",
    "        with open(test_list_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                data, label = line.split('\\t')\n",
    "                yield data, label\n",
    "\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建CNN网络\n",
    "\n",
    "def CNN_net(data,dict_dim, class_dim=10, emb_dim=128, hid_dim=128,hid_dim2=98):\n",
    "        emb = fluid.layers.embedding(input=data,\n",
    "                                 size=[dict_dim, emb_dim])\n",
    "        conv_3 = fluid.nets.sequence_conv_pool(\n",
    "                                                 input=emb,\n",
    "                                                 num_filters=hid_dim,\n",
    "                                                 filter_size=3,\n",
    "                                                 act=\"tanh\",\n",
    "                                                 pool_type=\"sqrt\")\n",
    "        conv_4 = fluid.nets.sequence_conv_pool(\n",
    "                                                 input=emb,\n",
    "                                                 num_filters=hid_dim2,\n",
    "                                                 filter_size=4,\n",
    "                                                 act=\"tanh\",\n",
    "                                                 pool_type=\"sqrt\")\n",
    "                                                 \n",
    "        output = fluid.layers.fc(\n",
    "            input=[conv_3, conv_4], size=class_dim, act='softmax')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义输入数据， lod_level不为0指定输入数据为序列数据\n",
    "words = fluid.layers.data(name='words', shape=[1], dtype='int64', lod_level=1)\n",
    "label = fluid.layers.data(name='label', shape=[1], dtype='int64')\n",
    "# 获取数据字典长度\n",
    "dict_dim = get_dict_len('/home/aistudio/data/dict_txt.txt')\n",
    "# 获取卷积神经网络\n",
    "# model = CNN_net(words, dict_dim, 15)\n",
    "# 获取分类器\n",
    "model = CNN_net(words, dict_dim)\n",
    "# 获取损失函数和准确率\n",
    "cost = fluid.layers.cross_entropy(input=model, label=label)\n",
    "avg_cost = fluid.layers.mean(cost)\n",
    "acc = fluid.layers.accuracy(input=model, label=label)\n",
    "\n",
    "# 获取预测程序\n",
    "test_program = fluid.default_main_program().clone(for_test=True)\n",
    "\n",
    "# 定义优化方法\n",
    "optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=0.002)\n",
    "opt = optimizer.minimize(avg_cost)\n",
    "\n",
    "# 创建一个执行器，CPU训练速度比较慢\n",
    "place = fluid.CPUPlace()\n",
    "# place = fluid.CUDAPlace(0)\n",
    "exe = fluid.Executor(place)\n",
    "# 进行参数初始化\n",
    "exe.run(fluid.default_startup_program())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取训练数据读取器和测试数据读取器\n",
    "train_reader = paddle.batch(reader=train_reader('/home/aistudio/data/train_list.txt'), batch_size=128)\n",
    "test_reader = paddle.batch(reader=test_reader('/home/aistudio/data/test_list.txt'), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据映射器\n",
    "feeder = fluid.DataFeeder(place=place, feed_list=[words, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass:0, Batch:0, Cost:2.32817, Acc:0.06250\n",
      "Pass:0, Batch:100, Cost:0.11346, Acc:0.96875\n",
      "Pass:0, Batch:200, Cost:0.14525, Acc:0.92188\n",
      "Test:0, Cost:0.11891, ACC:0.95387\n",
      "Pass:1, Batch:0, Cost:0.09609, Acc:0.97656\n",
      "Pass:1, Batch:100, Cost:0.08865, Acc:0.97656\n",
      "Pass:1, Batch:200, Cost:0.09323, Acc:0.96875\n",
      "Test:1, Cost:0.10305, ACC:0.96253\n",
      "Pass:2, Batch:0, Cost:0.08224, Acc:0.96875\n",
      "Pass:2, Batch:100, Cost:0.09027, Acc:0.96875\n",
      "Pass:2, Batch:200, Cost:0.09110, Acc:0.96875\n",
      "Test:2, Cost:0.09596, ACC:0.96600\n",
      "Pass:3, Batch:0, Cost:0.07155, Acc:0.96875\n",
      "Pass:3, Batch:100, Cost:0.03486, Acc:0.99219\n",
      "Pass:3, Batch:200, Cost:0.02980, Acc:0.98438\n",
      "Test:3, Cost:0.09496, ACC:0.96669\n",
      "Pass:4, Batch:0, Cost:0.05476, Acc:0.97656\n",
      "Pass:4, Batch:100, Cost:0.05107, Acc:0.98438\n",
      "Pass:4, Batch:200, Cost:0.05050, Acc:0.96875\n",
      "Test:4, Cost:0.09272, ACC:0.96756\n",
      "Pass:5, Batch:0, Cost:0.04230, Acc:0.98438\n",
      "Pass:5, Batch:100, Cost:0.11228, Acc:0.96875\n",
      "Pass:5, Batch:200, Cost:0.04500, Acc:0.98438\n",
      "Test:5, Cost:0.09163, ACC:0.96822\n",
      "Pass:6, Batch:0, Cost:0.02227, Acc:1.00000\n",
      "Pass:6, Batch:100, Cost:0.05358, Acc:0.98438\n",
      "Pass:6, Batch:200, Cost:0.04572, Acc:0.99219\n",
      "Test:6, Cost:0.09262, ACC:0.96869\n",
      "Pass:7, Batch:0, Cost:0.03775, Acc:0.98438\n",
      "Pass:7, Batch:100, Cost:0.02963, Acc:1.00000\n",
      "Pass:7, Batch:200, Cost:0.04557, Acc:0.99219\n",
      "Test:7, Cost:0.09317, ACC:0.96774\n",
      "Pass:8, Batch:0, Cost:0.04489, Acc:0.98438\n",
      "Pass:8, Batch:100, Cost:0.04277, Acc:0.99219\n",
      "Pass:8, Batch:200, Cost:0.02207, Acc:0.99219\n",
      "Test:8, Cost:0.09533, ACC:0.96793\n",
      "Pass:9, Batch:0, Cost:0.02844, Acc:0.99219\n",
      "Pass:9, Batch:100, Cost:0.01367, Acc:0.99219\n",
      "Pass:9, Batch:200, Cost:0.02528, Acc:0.99219\n",
      "Test:9, Cost:0.09493, ACC:0.96706\n",
      "训练模型保存完成！\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM=10\n",
    "model_save_dir = '/home/aistudio/work/infer_model/'\n",
    "# 开始训练\n",
    "\n",
    "for pass_id in range(EPOCH_NUM):\n",
    "    # 进行训练\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        train_cost, train_acc = exe.run(program=fluid.default_main_program(),\n",
    "                             feed=feeder.feed(data),\n",
    "                             fetch_list=[avg_cost, acc])\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print('Pass:%d, Batch:%d, Cost:%0.5f, Acc:%0.5f' % (pass_id, batch_id, train_cost[0], train_acc[0]))\n",
    "    # 进行测试\n",
    "    test_costs = []\n",
    "    test_accs = []\n",
    "    for batch_id, data in enumerate(test_reader()):\n",
    "        test_cost, test_acc = exe.run(program=test_program,\n",
    "                                              feed=feeder.feed(data),\n",
    "                                              fetch_list=[avg_cost, acc])\n",
    "        test_costs.append(test_cost[0])\n",
    "        test_accs.append(test_acc[0])\n",
    "    # 计算平均预测损失在和准确率\n",
    "    test_cost = (sum(test_costs) / len(test_costs))\n",
    "    test_acc = (sum(test_accs) / len(test_accs))\n",
    "    print('Test:%d, Cost:%0.5f, ACC:%0.5f' % (pass_id, test_cost, test_acc))\n",
    "\n",
    "# 保存预测模型\n",
    "if not os.path.exists(model_save_dir): \n",
    "    os.makedirs(model_save_dir) \n",
    "fluid.io.save_inference_model(model_save_dir, \n",
    "                            feeded_var_names=[words.name], \n",
    "                            target_vars=[model], \n",
    "                            executor=exe)\n",
    "print('训练模型保存完成！') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果标签为：0， 名称为：真新闻， 概率为：0.998947\n",
      "预测结果标签为：0， 名称为：真新闻， 概率为：0.969203\n"
     ]
    }
   ],
   "source": [
    "# 用训练好的模型进行预测并输出预测结果\n",
    "# 创建执行器\n",
    "place = fluid.CPUPlace()\n",
    "exe = fluid.Executor(place)\n",
    "exe.run(fluid.default_startup_program())\n",
    "\n",
    "save_path = '/home/aistudio/work/infer_model/'\n",
    "\n",
    "# 从模型中获取预测程序、输入数据名称列表、分类器\n",
    "[infer_program, feeded_var_names, target_var] = fluid.io.load_inference_model(dirname=save_path, executor=exe)\n",
    "\n",
    "\n",
    "# 获取数据\n",
    "def get_data(sentence):\n",
    "    # 读取数据字典\n",
    "    with open('/home/aistudio/data/dict_txt.txt', 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "    dict_txt = dict(dict_txt)\n",
    "    # 把字符串数据转换成列表数据\n",
    "    keys = dict_txt.keys()\n",
    "    data = []\n",
    "    for s in sentence:\n",
    "        # 判断是否存在未知字符\n",
    "        if not s in keys:\n",
    "            s = '<unk>'\n",
    "        data.append(int(dict_txt[s]))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = []\n",
    "# 获取图片数据\n",
    "data1 = get_data('战事一触即发？伊朗大规模海上军演向美示威，美将派更多军舰前往!!!!!|战事一触即...')\n",
    "data2 = get_data('大连.沈阳音乐学院？学校的脸呢？')\n",
    "data.append(data1)\n",
    "data.append(data2)\n",
    "\n",
    "# 获取每句话的单词数量\n",
    "base_shape = [[len(c) for c in data]]\n",
    "\n",
    "# 生成预测数据\n",
    "tensor_words = fluid.create_lod_tensor(data, base_shape, place)\n",
    "\n",
    "# 执行预测\n",
    "result = exe.run(program=infer_program,\n",
    "                 feed={feeded_var_names[0]: tensor_words},\n",
    "                 fetch_list=target_var)\n",
    "\n",
    "# 分类名称\n",
    "names = [ '真新闻', '假新闻']\n",
    "\n",
    "# 获取结果概率最大的label\n",
    "for i in range(len(data)):\n",
    "    lab = np.argsort(result)[0][i][-1]\n",
    "    print('预测结果标签为：%d， 名称为：%s， 概率为：%f' % (lab, names[lab], result[0][i][lab]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.6.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
